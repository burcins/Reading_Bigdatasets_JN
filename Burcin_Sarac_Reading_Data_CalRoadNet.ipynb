{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  California road network\n",
    "\n",
    "\n",
    "In this project, I worked on [California Road Network](https://www.cs.utah.edu/~lifeifei/SpatialDataset.htm) dataset as practice for reading and cleaning big datasets. I created csv files for further analysis, but in this part I only read the datasets and created csv files to make dataset ready for analysis. \n",
    "\n",
    "Datasets include \n",
    "1. California Road Network's Nodes (Node ID, Longitude, Latitude)\n",
    "2. California Road Network's Edges (Edge ID, Start Node ID, End Node ID, L2 Distance)\n",
    "3. California's Points of Interest With Original Category Name (Category Name, Longitude, Latitude)\n",
    "4. California's Points of Interest (Longitude, Latitude, Category ID)\n",
    "5. Merge Points of Interest with Road Network--Map Format\n",
    "For the 5th dataset, map format in detail is:\n",
    "        For each edge:\n",
    "        Start Node ID, End Node ID, Number of Points on This Edge, Edge Length. \n",
    "            For each point on this edge: \n",
    "            Category ID, Distance of This Point to the Start Node of This Edge  \n",
    "\n",
    "For the first four datasets, I used ``pd.read_csv()`` command from pandas, but for the last file, I created generators for reading data. With this my aim was to clean data while reading and also keep memory usage in minimum. I created 3 csv files from the 5th file. \n",
    "    First file called \"nodes_POIs.csv\" was for spliting beginning node and related POIs in each line, but this dataset was also needed to be cleaned. So I created second csv as cleaned version of the first one which called \"nodes_POIs2.csv\". This dataset splits all POIs into different lines with keeping their beggining node as first in line. And as last csv file called \"edge_lengths.csv\" I only took into account beggining and end nodes of edges and the legth of them and also number of POIs on these edges while creating csv rows. \n",
    "\n",
    "---\n",
    "\n",
    "> Burcin Sarac <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started with importing required packages and setting working directory of mine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"E:/dersler/3.Spring/mining big datasets/Assignment_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Dataset\n",
    "\n",
    "Saved as \"nodes.csv\" to the working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv(\"https://www.cs.utah.edu/~lifeifei/research/tpq/cal.cnode\", sep=' ', \n",
    "                    names= [\"node_ID\", \"longitude\", \"latitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_ID</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-121.904167</td>\n",
       "      <td>41.974556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-121.902153</td>\n",
       "      <td>41.974766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-121.896790</td>\n",
       "      <td>41.988075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-121.889603</td>\n",
       "      <td>41.998032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-121.886681</td>\n",
       "      <td>42.008739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   node_ID   longitude   latitude\n",
       "0        0 -121.904167  41.974556\n",
       "1        1 -121.902153  41.974766\n",
       "2        2 -121.896790  41.988075\n",
       "3        3 -121.889603  41.998032\n",
       "4        4 -121.886681  42.008739"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.to_csv(\"nodes.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Dataset\n",
    "\n",
    "Saved as \"edges.csv\" to the working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.read_csv(\"https://www.cs.utah.edu/~lifeifei/research/tpq/cal.cedge\", sep=' ', \n",
    "                    names= [\"edge_ID\", \"startnodeID\", \"endnodeID\", \"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edge_ID</th>\n",
       "      <th>startnodeID</th>\n",
       "      <th>endnodeID</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.005952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.014350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.012279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.011099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   edge_ID  startnodeID  endnodeID  distance\n",
       "0        0            0          1  0.002025\n",
       "1        1            0          6  0.005952\n",
       "2        2            1          2  0.014350\n",
       "3        3            2          3  0.012279\n",
       "4        4            3          4  0.011099"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.to_csv(\"edges.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "catnames = pd.read_csv(\"https://www.cs.utah.edu/~lifeifei/research/tpq/CA\", sep=' ', \n",
    "                    names= [\"categoryname\", \"longitude\", \"latitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categoryname</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>airport</td>\n",
       "      <td>-114.18639</td>\n",
       "      <td>34.30806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>airport</td>\n",
       "      <td>-114.43083</td>\n",
       "      <td>34.52750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>airport</td>\n",
       "      <td>-114.52667</td>\n",
       "      <td>33.86944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>airport</td>\n",
       "      <td>-114.57528</td>\n",
       "      <td>34.18389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>airport</td>\n",
       "      <td>-114.60194</td>\n",
       "      <td>34.81944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  categoryname  longitude  latitude\n",
       "0      airport -114.18639  34.30806\n",
       "1      airport -114.43083  34.52750\n",
       "2      airport -114.52667  33.86944\n",
       "3      airport -114.57528  34.18389\n",
       "4      airport -114.60194  34.81944"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catnames.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois = pd.read_csv(\"https://www.cs.utah.edu/~lifeifei/research/tpq/caldata\", sep=' ', \n",
    "                    names= [\"longitude\", \"latitude\", \"catID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>catID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-114.186394</td>\n",
       "      <td>34.308060</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-114.430832</td>\n",
       "      <td>34.527500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-114.526672</td>\n",
       "      <td>33.869438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-114.575279</td>\n",
       "      <td>34.183891</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-114.601936</td>\n",
       "      <td>34.819439</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    longitude   latitude  catID\n",
       "0 -114.186394  34.308060      0\n",
       "1 -114.430832  34.527500      0\n",
       "2 -114.526672  33.869438      0\n",
       "3 -114.575279  34.183891      0\n",
       "4 -114.601936  34.819439      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pois.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging 3th and 4th dataset as one\n",
    "\n",
    "In this step I merged category id numbers with category names by using similarity between latitude and longitude values, but since the values has different digit numbers, I rounded them first. Afterwards, I filtered duplications and saved it as \"categories.csv\" to the working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_cat = pd.merge(pois, catnames,  how='outer', left_on=[round(pois.iloc[:,0],5),round(pois.iloc[:,1],5)], \n",
    "                      right_on = [round(catnames.iloc[:,1],5),\n",
    "                                  round(catnames.iloc[:,2],5)]).drop([\"longitude_x\",\"latitude_x\",\n",
    "                                                                          \"longitude_y\",\"latitude_y\"], \n",
    "                                                                     axis=1).drop_duplicates(\"key_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = merged_cat.drop([\"key_0\", \"key_1\"], axis=1).drop_duplicates(\"catID\").reset_index(drop=True).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catID</th>\n",
       "      <th>categoryname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>arch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>arroyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>bar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   catID categoryname\n",
       "0    0.0      airport\n",
       "1    1.0         arch\n",
       "2    2.0         area\n",
       "3    3.0       arroyo\n",
       "4    4.0          bar"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.to_csv(\"categories.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset\n",
    "\n",
    "For this dataset, as I mentioned in the beginning part, I created generators to read lines one by one instead of importing all dataset and load to memory. But first I downloaded the link and save it as original txt file with name \"calmap.txt\". \n",
    "Saved as \"catnames.csv\" to the working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "outF = open(\"calmap.txt\",\"w\")\n",
    "outF.writelines(requests.get(\"https://www.cs.utah.edu/~lifeifei/research/tpq/calmap.txt\").text)\n",
    "\n",
    "outF.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, I created first generator below, which read the txt file by 2 lines in each step and while reading first line it merges numbers by using ``join()`` function. After that, it checks with the helps of if condition and filter the line if 4th number in each line is above 1(>1) in \"calmap.txt\" file, and then it merges beginning node, POIid and POI distance together by putting them into a line if condition was met.  With this process I created \"nodes_POIs.csv\" file, which includes beginning node ID and all POIs with distance values to the beginning node line by line. \n",
    "\n",
    "In second step, I splited all POIs into separate rows with their beginning node id numbers. \n",
    "\n",
    "For instance;\n",
    "\n",
    "    In first step first line created like this;\n",
    "        1,25,0.00537134,55,0.0040002,\n",
    "        \n",
    "    And in second step I converted it into this;\n",
    "        1,25,0.00537134\n",
    "        1,55,0.0040002\n",
    "        \n",
    "And at last step I just determined column names and save it again to the working directory. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(\"nodes_POIs.csv\", \"w\")\n",
    "\n",
    "with open(\"calmap.txt\",\"r\") as f:\n",
    "    for line1,line2 in itertools.zip_longest(*[f]*2):\n",
    "        line1= ''.join(line1)\n",
    "        if re.split(r\"\\s\", str(line1))[3]!='':\n",
    "            if float(re.split(r\"\\s\", str(line1))[3])>=1:\n",
    "                nodes_pois = ('{0},{1}'.format(re.split(r\"\\s\", str(line1))[0], ','.join(re.split(r\"\\s\", str(line2)))))\n",
    "                outF.writelines(nodes_pois + '\\n')\n",
    "\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(\"nodes_POIs2.csv\", \"w\")\n",
    "\n",
    "with open(\"nodes_POIs.csv\",\"r\") as f:\n",
    "    for line in f:\n",
    "        for i in range(0,(len(re.split(\",\", str(line)))-2),2):\n",
    "            if re.split(r\",\",str(line))[i+1] != '':\n",
    "                nodes_pois = ('{0},{1},{2}'.format(re.split(\",\", str(line))[0],re.split(\",\", str(line))[i+1], \n",
    "                                           re.split(r\",\", str(line))[i+2]))\n",
    "                outF.writelines(nodes_pois + '\\n')\n",
    "\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodespois = pd.read_csv(\"nodes_POIs2.csv\", names= [\"startnodeID\", \"catID\", \"cat_distance\"])\n",
    "nodespois.to_csv(\"nodes_POIs2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all I created edge_lenths data by only reading lines if 4th number in each line in \"calmap.txt\" dataset got values between 0 and 1. And append all lines which satisfied this condition with including their Start node, end node, edge lenth and number of POIs on that edge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outF = open(\"edge_lengths.csv\", \"w\")\n",
    "\n",
    "with open(\"calmap.txt\",\"r\") as f:\n",
    "    for line1 in itertools.zip_longest(f):\n",
    "        line1= ''.join(line1)\n",
    "        if re.split(r\"\\s\", str(line1))[3]!='':\n",
    "            if not 0<float(re.split(r\"\\s\", str(line1))[3])<1:\n",
    "                edgelengths = (','.join(re.split(r\"\\s\", str(line1))))\n",
    "                outF.writelines(edgelengths+'\\n')\n",
    "                \n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgedetails = pd.read_csv(\"edge_lengths.csv\", names= [\"startnode\",\"endnode\", \"distance\", \"nr_of_pois\"])\n",
    "edgedetails.to_csv(\"edge_lengths.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after every documents had read, I used the command below to drop memory usage of Jupyter Notebook by decreasing its display limit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
